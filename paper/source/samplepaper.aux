\relax 
\citation{donahue2014decaf}
\citation{ma2015multimodal}
\citation{tome2016deep}
\citation{deng2009imagenet}
\citation{krizhevsky2012imagenet}
\citation{simonyan2014very}
\citation{szegedy2016rethinking}
\citation{chollet2016xception}
\citation{he2016deep}
\citation{huang2017densely}
\citation{parry2005digital}
\citation{allen2000collaboration}
\citation{mensink2014rijksmuseum}
\citation{mensink2014rijksmuseum}
\@writefile{toc}{\contentsline {title}{Deep Transfer Learning for Art Classification Problems}{1}}
\@writefile{toc}{\authcount {4}}
\@writefile{toc}{\contentsline {author}{Matthia Sabatelli \and Mike Kestemont \and Walter Daelemans \and Pierre Geurts}{1}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction and Related Work}{1}}
\citation{huang2007labeled}
\citation{stallkamp2011german}
\citation{tajbakhsh2016convolutional}
\citation{van2015deep}
\citation{reyes2015fine}
\citation{ackermann2018using}
\citation{razavian2014cnn}
\citation{kornblith2018better}
\citation{kornblith2018better}
\citation{pan2010survey}
\citation{pan2010survey}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{3}}
\newlabel{sec:methods}{{2}{3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Transfer Learning}{3}}
\newlabel{subsec: tl}{{2.1}{3}}
\newlabel{loss}{{1}{3}}
\citation{weibel1998dublin}
\citation{simonyan2014very}
\citation{szegedy2016rethinking}
\citation{chollet2016xception}
\citation{xie2017aggregated}
\citation{chollet2015keras}
\citation{abadi2016tensorflow}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Datasets and Classification Challenges}{4}}
\newlabel{subsec:datasets}{{2.2}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Neural Architectures and Classification Approaches}{4}}
\newlabel{subsec: neural_nets}{{2.3}{4}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces An overview of the two datasets that are used in our experiments. Each color of the table corresponds to a different classification challenge, starting from challenge $1$ which is represented in yellow, challenge $2$ in blue and finally challenge $3$ in red. Furthermore we represent with $N_t$ the amount of samples constituting the datasets and with $Q_t$ the number of labels. Lastly, we also report if there are common labels between the two heritage collections.\relax }}{5}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{table:dataset_overview}{{1}{5}}
\citation{masters2018revisiting}
\citation{tieleman2012lecture}
\citation{caruana2001overfitting}
\citation{kornblith2018better}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces A visualization of the images that are used for our experiments. It is possible to see how the samples range from images representing plates made of porcelain to violins, and from Japanese artworks to a more simple picture of a key.\relax }}{6}}
\newlabel{fig:datasets}{{1}{6}}
\citation{he2015delving}
\citation{bidoiadeep}
\@writefile{toc}{\contentsline {section}{\numberline {3}Results}{7}}
\newlabel{sec:results}{{3}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}From Natural to Art Images}{7}}
\newlabel{subsec: natural_to_art}{{3.1}{7}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Comparison between the fine tuning approach versus the off the shelf one when classifying the material of the heritage objects of the Rijksmuseum dataset. We observe how the first approach (as reported by the the dashed lines) leads to significant improvements when compared to the latter one (reported by the dash-dotted lines) for three out of four neural architectures. Furthermore, we can also observe how training a DCNN from scratch leads to worse results when compared to fine-tuned architectures which have been pre-trained on ImageNet (solid orange line).\relax }}{8}}
\newlabel{fig:rijks_material}{{2}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces A similar analysis as the one which has been reported in Figure 2\hbox {} but for the second and third classification challenges (left and right figures respectively). The results show again the significant benefits that fine tuning (reported by the dashed line plots) has when compared to the off the shelf approach (reported by the dash-dotted lines) and how this latter strategy miserably under-performs when it comes to artist classification. Furthermore we again see the benefits that using a pre-trained DCNN has over training the architecture from scratch (solid orange line).\relax }}{10}}
\newlabel{fig:type_and_artist}{{3}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Discussion}{10}}
\newlabel{subsec: RijksDiscussion}{{3.2}{10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}From One Art Collection to Another}{10}}
\newlabel{subsec: from_one_to_another}{{3.3}{10}}
\citation{bojarski2016visualbackprop}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces An overview of the results obtained by the different DCNNs on the testing set when classifying the heritage objects of the Rijksmuseum. Bold results report the best performing architectures overall. The additional columns ``Params'' and ``$\EuScript  {X}$'' report the amount of parameters the ANNs have to learn and the size of the feature vector which is used as input for the softmax classifier.\relax }}{11}}
\newlabel{tab:Rijksmuseum_Dataset}{{2}{11}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces The results obtained on the classification experiments performed on the Antwerp dataset with DCNNs which have been initially pre-trained on ImageNet ($\theta $) and the same architectures which have been fine tuned on the Rijksmuseum dataset ($\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \theta $}\mathaccent "0362{\theta }$). Our results show how the latter pre-trained DCNNs yield better results both if used as off the shelf feature extractors and if fine tuned.\relax }}{12}}
\newlabel{tab:Antwerpen_dataset}{{3}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Selective Attention}{12}}
\citation{stamatatos:2009}
\citation{wollheim:1972}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces A visualization that shows the differences between which sets of pixels in an image are considered informative for a DCNN which is only pre-trained on ImageNet, compared to the same architecture which has also been fine tuned on the Rijksmuseum collection. It is clear how the latter neural network develops novel selective attention mechanisms over the original image.\relax }}{13}}
\newlabel{fig:saliency_maps}{{4}{13}}
\citation{huang2017densely}
\bibstyle{splncs04}
\bibdata{mybibliography.bib}
\bibcite{abadi2016tensorflow}{1}
\bibcite{ackermann2018using}{2}
\bibcite{allen2000collaboration}{3}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{14}}
\newlabel{sec:conclusion}{{4}{14}}
\bibcite{bidoiadeep}{4}
\bibcite{bojarski2016visualbackprop}{5}
\bibcite{caruana2001overfitting}{6}
\bibcite{chollet2016xception}{7}
\bibcite{chollet2015keras}{8}
\bibcite{deng2009imagenet}{9}
\bibcite{donahue2014decaf}{10}
\bibcite{stamatatos:2009}{11}
\bibcite{he2015delving}{12}
\bibcite{he2016deep}{13}
\bibcite{huang2017densely}{14}
\bibcite{huang2007labeled}{15}
\bibcite{kornblith2018better}{16}
\bibcite{krizhevsky2012imagenet}{17}
\bibcite{ma2015multimodal}{18}
\bibcite{masters2018revisiting}{19}
\bibcite{mensink2014rijksmuseum}{20}
\bibcite{pan2010survey}{21}
\bibcite{parry2005digital}{22}
\bibcite{razavian2014cnn}{23}
\bibcite{reyes2015fine}{24}
\bibcite{simonyan2014very}{25}
\bibcite{stallkamp2011german}{26}
\bibcite{szegedy2016rethinking}{27}
\bibcite{tajbakhsh2016convolutional}{28}
\bibcite{tieleman2012lecture}{29}
\bibcite{tome2016deep}{30}
\bibcite{weibel1998dublin}{31}
\bibcite{van2015deep}{32}
\bibcite{wollheim:1972}{33}
\bibcite{xie2017aggregated}{34}
